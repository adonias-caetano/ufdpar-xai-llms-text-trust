{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5845476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import tokenizer_utils\n",
    "def do_nothing(*args, **kwargs):\n",
    "    pass\n",
    "tokenizer_utils.fix_untrained_tokens = do_nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d846acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major: 8, Minor: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.10: Fast Qwen3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.508 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78f5cf29d484563838ad8ab70a2b53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Models that have been tested\n",
    "used_models = [\n",
    "    \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\"\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",   \n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"    \n",
    "]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None #None for auto detection\n",
    "\n",
    "model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\";load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb259c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4096])\n",
      "torch.Size([151936, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{15: 0, 16: 1, 17: 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_token_ids = []\n",
    "for i in range(0, NUM_CLASSES+1):\n",
    "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
    "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
    "\n",
    "old_shape = model.lm_head.weight.shape\n",
    "old_size = old_shape[0]\n",
    "print(par.shape)\n",
    "print(old_shape)\n",
    "\n",
    "model.lm_head.weight = par\n",
    "\n",
    "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} \n",
    "reverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20da53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedro/joaopedro/llm/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1222: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
      "  warnings.warn(msg)\n",
      "Unsloth 2025.10.10 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n",
      "trainable parameters: 43659264\n"
     ]
    }
   ],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"lm_head\",\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    ")\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ba6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/joaopedro/joaopedro/llm/Mestrado/Dados/dataset_limpo.csv\") #columns are text,label\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(len(train_df))\n",
    "\n",
    "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
    "a = plt.hist(token_counts, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d85978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Aqui estÃ¡ uma sentenÃ§a.\n",
    "{}\n",
    "\n",
    "Se a sentenÃ§a contÃ©m ideaÃ§Ã£o suicida, responda \"Sim\". Caso contrÃ¡rio, responda \"NÃ£o\":\n",
    "\n",
    "A resposta correta Ã©: \"{}\"\"\"\n",
    "\n",
    "positivelabel = \"Sim\" #Yes\n",
    "negativelabel = \"NÃ£o\" #No\n",
    "\n",
    "def formatting_prompts_func(dataset_):\n",
    "    texts = []\n",
    "    for i in range(len(dataset_['text'])):\n",
    "        text_ = dataset_['text'].iloc[i]\n",
    "        label_ = dataset_['label'].iloc[i] \n",
    "\n",
    "        text = prompt.format(text_, label_)\n",
    "\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "#apply formatting_prompts_func to train_df\n",
    "train_df['text'] = formatting_prompts_func(train_df)\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309550a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
    "\n",
    "        return batch\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a4fe9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_labels_llm(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    max_seq_length,\n",
    "    batch_size=32,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #logits do ÃšLTIMO token\n",
    "        logits = outputs.logits[:, -1, :2]\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_preds.append(preds.cpu())\n",
    "\n",
    "    return (\n",
    "        torch.cat(all_preds).numpy(),\n",
    "        torch.cat(all_probs).numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# =========================\n",
    "# ConfiguraÃ§Ãµes\n",
    "# =========================\n",
    "NUM_FOLDS = 5\n",
    "SEED = 3407\n",
    "BASE_OUTPUT_DIR = \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/Teste_8B\"\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "full_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "all_fold_metrics = []\n",
    "\n",
    "# =========================\n",
    "# Cross-validation\n",
    "# =========================\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    skf.split(full_dataset[\"text\"], full_dataset[\"label\"])\n",
    "):\n",
    "    fold_id = fold + 1\n",
    "    print(f\"\\nðŸš€ Fold {fold_id}/{NUM_FOLDS}\")\n",
    "\n",
    "    fold_dir = os.path.join(BASE_OUTPUT_DIR, f\"fold_{fold_id}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    train_dataset = full_dataset.select(train_idx)\n",
    "    val_dataset   = full_dataset.select(val_idx)\n",
    "\n",
    "    # =========================\n",
    "    # Treinamento\n",
    "    # =========================\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        output_dir=os.path.join(fold_dir, \"model\"),\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=1,\n",
    "        args=training_args,\n",
    "        data_collator=collator,\n",
    "        dataset_text_field=\"text\",\n",
    "        formatting_func=formatting_prompts_func,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # =========================\n",
    "    # InferÃªncia (Ãºltimo token)\n",
    "    # =========================\n",
    "    y_true = np.array(val_dataset[\"label\"])\n",
    "\n",
    "    y_pred_raw, y_prob = predict_labels_llm(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        val_dataset[\"text\"],\n",
    "        max_seq_length=max_seq_length,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    y_score = y_prob[:, 1]  #probabilidade da classe positiva\n",
    "\n",
    "    # =========================\n",
    "    # Threshold Ã³timo (PR â†’ F1)\n",
    "    # =========================\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    optimal_idx = np.argmax(f1_scores[:-1])\n",
    "\n",
    "    #pr_thresholds tem tamanho len(precision) - 1\n",
    "    optimal_threshold = pr_thresholds[optimal_idx]\n",
    "\n",
    "    y_pred = (y_score >= optimal_threshold).astype(int)\n",
    "\n",
    "    # =========================\n",
    "    # MÃ©tricas\n",
    "    # =========================\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"auc\": roc_auc_score(y_true, y_score),\n",
    "        \"optimal_threshold\": float(optimal_threshold),\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸŽ¯ Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"ðŸ“Š MÃ©tricas Fold {fold_id}: {metrics}\")\n",
    "\n",
    "    all_fold_metrics.append(metrics)\n",
    "\n",
    "    # =========================\n",
    "    # Salvar mÃ©tricas\n",
    "    # =========================\n",
    "    with open(os.path.join(fold_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # =========================\n",
    "    # Curvas ROC e PR\n",
    "    # =========================\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_score)\n",
    "\n",
    "    np.savez(\n",
    "        os.path.join(fold_dir, \"roc_pr_data.npz\"),\n",
    "        y_true=y_true,\n",
    "        y_score=y_score,\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        roc_thresholds=roc_thresholds,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        pr_thresholds=pr_thresholds,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MÃ©dia final\n",
    "# =========================\n",
    "final_metrics = {}\n",
    "std_metrics   = {}\n",
    "\n",
    "for key in all_fold_metrics[0]:\n",
    "    values = [m[key] for m in all_fold_metrics]\n",
    "    final_metrics[key] = float(np.mean(values))\n",
    "    std_metrics[key]   = float(np.std(values))\n",
    "\n",
    "print(\"\\nâœ… MÃ‰TRICAS FINAIS (Cross-Validation)\")\n",
    "for k in final_metrics:\n",
    "    print(f\"{k.upper():15s}: {final_metrics[k]:.4f} Â± {std_metrics[k]:.4f}\")\n",
    "\n",
    "summary = {\n",
    "    \"mean\": final_metrics,\n",
    "    \"std\": std_metrics,\n",
    "    \"per_fold\": {\n",
    "        f\"fold_{i+1}\": all_fold_metrics[i]\n",
    "        for i in range(NUM_FOLDS)\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(os.path.join(BASE_OUTPUT_DIR, \"summary_metrics.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e9035",
   "metadata": {},
   "source": [
    "Resultados Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fb2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model  Accuracy (mean)  Accuracy (std)  Precision (mean)  \\\n",
      "4   Qwen-14B         0.988753        0.012297          0.976595   \n",
      "0  Qwen-600M         0.980149        0.016460          0.959180   \n",
      "1  Qwen-1.7B         0.966558        0.019576          0.936621   \n",
      "2    Qwen-4B         0.960618        0.028433          0.901036   \n",
      "3    Qwen-8B         0.955010        0.045170          0.898908   \n",
      "\n",
      "   Precision (std)  Recall (mean)  Recall (std)  F1 (mean)  F1 (std)  \\\n",
      "4         0.027090       0.985136      0.016399   0.980769  0.020809   \n",
      "0         0.044799       0.974791      0.009261   0.966526  0.026899   \n",
      "1         0.036955       0.949504      0.051723   0.942137  0.035221   \n",
      "2         0.063990       0.974831      0.021543   0.935894  0.044699   \n",
      "3         0.102463       0.967967      0.030775   0.929382  0.065898   \n",
      "\n",
      "   AUC (mean)  AUC (std)  \n",
      "4    0.996465   0.002593  \n",
      "0    0.992707   0.005327  \n",
      "1    0.988740   0.007355  \n",
      "2    0.980416   0.014141  \n",
      "3    0.977704   0.027730  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "MODEL_DIRS = {\n",
    "    \"Qwen-600M\": \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/results_Qwen_600M\",\n",
    "    \"Qwen-1.7B\": \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/results_Qwen_1.7B\",\n",
    "    \"Qwen-4B\":   \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/results_Qwen_4B\",\n",
    "    \"Qwen-8B\":   \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/results_Qwen_8B_Teste1\",\n",
    "    \"Qwen-14B\":  \"/home/joaopedro/joaopedro/llm/Mestrado/Resultados/results_Qwen_14BB\",\n",
    "}\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, base_dir in MODEL_DIRS.items():\n",
    "    summary_path = os.path.join(base_dir, \"summary_metrics.json\")\n",
    "\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    mean_metrics = summary[\"mean\"]\n",
    "    std_metrics  = summary[\"std\"]\n",
    "\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy (mean)\": mean_metrics.get(\"accuracy\"),\n",
    "        \"Accuracy (std)\": std_metrics.get(\"accuracy\"),\n",
    "        \"Precision (mean)\": mean_metrics.get(\"precision\"),\n",
    "        \"Precision (std)\": std_metrics.get(\"precision\"),\n",
    "        \"Recall (mean)\": mean_metrics.get(\"recall\"),\n",
    "        \"Recall (std)\": std_metrics.get(\"recall\"),\n",
    "        \"F1 (mean)\": mean_metrics.get(\"f1\"),\n",
    "        \"F1 (std)\": std_metrics.get(\"f1\"),\n",
    "        \"AUC (mean)\": mean_metrics.get(\"auc\"),\n",
    "        \"AUC (std)\": std_metrics.get(\"auc\"),\n",
    "    }\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "df_metrics = df_metrics.sort_values(\"AUC (mean)\", ascending=False)\n",
    "\n",
    "print(df_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
